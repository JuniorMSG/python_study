"""
    subject
        Machine_Running
    topic
        ë§ˆì¼€íŒ… ë°ì´í„° ì‹¤ìŠµ

    Describe


        axes : https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html
        x = np.linspace(0, 2 * np.pi, 400)
        y = np.sin(x ** 2)
        ax_temp.scatter(x, y)

    Contens
        01.
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns




def marketing_01():
    """
        subject
            Machine_Running
        topic
            ex4. Kaggle_Data_Set : Advertising
        content
            01. ë°ì´í„° íƒìƒ‰
        Describe

            Kaggle Data_Set
                DATA
                TV      - TV ë§¤ì²´ë¹„
                Radio   - ë¼ë””ì˜¤ ë§¤ì²´ë¹„
                News    - ì‹ ë¬¸ ë§¤ì²´ë¹„
                sales   - ë§¤ì¶œì•¡

                ë¬¸ì œ ì •ì˜
                    ì „ì œ
                        ì‹¤ì œë¡œëŠ” ê´‘ê³  ë§¤ì²´ë¹„ ì´ì™¸ì˜ ë§ì€ ìš”ì¸ì´ ë§¤ì¶œì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. (ì˜ì—…ì¸ë ¥ ìˆ˜ ,ì…ì†Œë¬¸, ê²½ê¸°, ìœ í–‰ ë“±..)
                        ë¶„ì„ì—ì„œëŠ” ë‹¤ë¥¸ ìš”ì¸ì´ ëª¨ë‘ ë™ì¼í•œ ìƒí™©ì—ì„œ ë§¤ì²´ë¹„ë§Œ ë³€ê²½í–ˆì„ ë•Œ ë§¤ì¶œì•¡ì˜ ë³€í™”ê°€ ë°œìƒí•œ ê²ƒì´ë¼ê³  ê°„ì£¼
                        ì‹¤ì œë¡œ Acquisition ë‹¨ê³„ì—ì„œëŠ” ì¢…ì†ë³€ìˆ˜ê°€ ë§¤ì¶œì•¡ë³´ë‹¤ëŠ” ë°©ë¬¸ììˆ˜, ê°€ì…ììˆ˜, DAU, MAUë“±ì˜ ì§€í‘œê°€ ë  ê²ƒ.
                        2011ë…„ ë°ì´í„°ì„
                    ë¶„ì„ì˜ ëª©ì 
                        ê° ë¯¸ë””ì–´ë³„ë¡œ ë§¤ì²´ë¹„ë¥¼ ì–´ë–»ê²Œ ì“°ëŠëƒì— ë”°ë¼ì„œ ë§¤ì¶œì•¡ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆì§€ ì˜ˆì¸¡
                        ê¶ê·¹ì ìœ¼ë¡œëŠ” ë§¤ì¶œì•¡ì„ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” ë¯¸ë””ì–´ ë¯¹ìŠ¤ì˜ êµ¬ì„±ì„ ë„ì¶œ
                        ì´ ë¯¸ë””ì–´ë¯¹ìŠ¤ëŠ” í–¥í›„ ë¯¸ë””ì–´ í”Œëœì„ ìˆ˜ë¦½í•  ë•Œ ì‚¬ìš© ë  ìˆ˜ ìˆë‹¤.

        sub Contents
            01.
    """
    # DATA                  - ì¬ë£Œ
    # Processing Algorithm  - ì¡°ë¦¬ë²•  (Python, ê°•í™”í•™ìŠµ ë“±ë“±..)
    # Application           - ìš”ë¦¬    (ì•ŒíŒŒê³ , ì±—ë´‡, ì´ëŸ°ì €ëŸ° ì–´í”Œë¦¬ì¼€ì´ì…˜ ë“±ë“±)
    # AARRR & (RARRA ì¤‘ìš”ë„ì— ë”°ë¼ì„œ ì¡°ê¸ˆì”© ë‹¤ë¦„)

    # Acquisition(ì‚¬ìš©ìíšë“), Activation (ì‚¬ìš©ì í™œì„±í™”), Retention(ì‚¬ìš©ì ìœ ì§€), Revenue(ë§¤ì¶œ), Referral(ì¶”ì²œ)

    # Activation(ì‚¬ìš©ì í™œì„±í™”)   : ì‚¬ìš©ìê°€ ì–´ë–»ê²Œ ì„œë¹„ìŠ¤ë¥¼ ì ‘í•˜ëŠ”ê°€ ? ( DAU, MAU, New User, ë°©ë¬¸ì ìˆ˜ ë“±)
    # Acquisition(ì‚¬ìš©ìíšë“)    : ì‚¬ìš©ìê°€ ì²˜ìŒ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í–ˆì„ ë•Œ ê²½í—˜ì´ ì¢‹ì•˜ëŠ”ê°€?  ( Avg, PV, Avg. Duration, ê°€ì…ì ìˆ˜ ë“±)
    # Retention(ì‚¬ìš©ì ìœ ì§€) : ì‚¬ìš©ìê°€ ìš°ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ê³„ì† ì´ìš©í•˜ëŠ”ê°€? ( Retention Rate )
    # Revenue(ë§¤ì¶œ) : ì–´ë–»ê²Œ ëˆì„ ë²„ëŠ”ê°€ ? ( Conversion )
    # Referral(ì¶”ì²œ) : ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì—ê²Œ ì œí’ˆì„ ì†Œê°œí•˜ëŠ”ê°€? (SNS Share Rate)

    # STEP 1. ë¯¸ë””ì–´ë³„ ê´‘ê³ ë¹„ EDA
    # STEP 2. ë¶„ì„ ëª¨ë¸ë§ ë§¤ì²´ë¹„ë¡œ ì„¸ì¼ì¦ˆ ì˜ˆì¸¡
    # STEP 3. ë¶„ì„ ê²°ê³¼ í•´ì„ ì ìš© ë°©ì•ˆ

    print("\n", "=" * 5, "01", "=" * 5)
    df = pd.read_csv("./data_file/Advertising.csv")
    """
        ê¸°ë³¸ ì²´í¬ ì‘ì—…
        1. ë°ì´í„° í™•ì¸   : df.shape, df.head(), df.tail()
        2. ê²°ì¸¡ê°’ ì¸¡ì •   : df.info(), df.isnull().sum()
        3. ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        4. ê¸°ìˆ í†µê³„ í™•ì¸ : df.describe()
        5. ë³€ìˆ˜ê°„ì˜ correlation í™•ì¸
        6. ë³€ìˆ˜ê°„ì˜ pairplot í™•ì¸
        7. Label, Feature(ì¸í’‹ ë³€ìˆ˜, ë…ë¦½ ë³€ìˆ˜) ì§€ì •
        
    """

    fig, axes = plt.subplots(2, 4, sharey=False, tight_layout=True, figsize=(15, 6), num='view')
    # fig = plt.figure(tight_layout=True, figsize=(15, 6), num='view')

    # ë°ì´í„° í™•ì¸
    print(df.shape)
    print(df.tail())
    # ê²°ì¸¡ê°’ ì¸¡ì •
    print(df.info())
    print(df.isnull().sum())

    # ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    df = df[['TV', 'radio', 'newspaper', 'sales']]

    # ê¸°ìˆ í†µê³„ í™•ì¸ : df.describe()
    print(df.describe())

    # ë³€ìˆ˜ê°„ì˜ correlation í™•ì¸ df.corr() ì‹œê°í™”
    corr = df.corr()
    print(corr)
    # annot=True ìˆ«ì í‘œì‹œí•´ì¤Œ

    ax_temp = axes[0, 0]
    ax_temp.set_title('HeatMap')
    sns.heatmap(corr, annot=True, ax=ax_temp)

    ax_temp = axes[0, 1]
    ax_temp.set_title('Scatter TV')
    sns.scatterplot(data=df, x='TV', y='sales', ax=ax_temp)

    ax_temp = axes[0, 2]
    ax_temp.set_title('Scatter Radio')
    sns.scatterplot(data=df, x='radio', y='sales', ax=ax_temp)

    ax_temp = axes[0, 3]
    ax_temp.set_title('Scatter News')
    sns.scatterplot(data=df, x='newspaper', y='sales', ax=ax_temp)




    # 6. ë³€ìˆ˜ê°„ì˜ pairplot ì¶œë ¥
    sns.pairplot(df[['TV', 'radio', 'newspaper', 'sales']])

    # 7. Label, Feature(ì¸í’‹ ë³€ìˆ˜, ë…ë¦½ ë³€ìˆ˜) ì§€ì •
    Labels = df['sales']
    features = df[['TV', 'radio', 'newspaper']]
    print(Labels.shape)
    print(features.shape)




    print("\n", "=" * 3, "01.", "=" * 3)

    # ì„ í˜•íšŒê·€ ë¶„ì„ (stats model)
    import statsmodels.formula.api as sm
    model1 = sm.ols(formula='sales ~ TV + radio + newspaper', data=df).fit()
    print(model1.summary())

    # ë¶„ì„ì´ ì˜ëœê²ƒì¸ê°€ ?
    # R-squared: 0.897 ë†’ì„ ìˆ˜ë¡ ì¢‹ì§€ë§Œ ë„ˆë¬´ ë†’ìœ¼ë©´ ë¬´ì–¸ê°€ ì˜ëª» ëœê²ƒ..
    # P>|t| (P-value) í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ê°€ ìˆëŠ” ê°’ì¸ê°€? (0.05 ì´ìƒì´ë©´ ìœ ì˜í•˜ì§€ ì•Šë‹¤.)
    # coef

    # ì„ í˜•íšŒê·€ ë¶„ì„ (sklearn model)
    from sklearn.datasets import make_regression
    from sklearn.linear_model import LinearRegression
    model2 = LinearRegression().fit(features, Labels)
    print(model2.intercept_ , model2.coef_)

    print("\n", "=" * 3, "02.", "=" * 3)

    model1 = sm.ols(formula='sales ~ TV + radio + newspaper', data=df).fit()
    model2 = sm.ols(formula='sales ~ TV + radio', data=df).fit()
    model3 = sm.ols(formula='sales ~ TV', data=df).fit()

    dict_data = {'TV' : 300, 'radio' : 10, 'newspaper' : 4}
    model1_pred = model1.predict({'TV' : 300, 'radio' : 10, 'newspaper' : 4})
    print(model1_pred)
    pred = 2.9389 + 0.0458 * dict_data['TV'] + 0.1885 * dict_data['radio'] - 0.0010 * dict_data['newspaper']
    print(pred)
    """
        Intercept      2.9389      0.312      9.422      0.000       2.324       3.554
        TV             0.0458      0.001     32.809      0.000       0.043       0.049
        radio          0.1885      0.009     21.893      0.000       0.172       0.206
        newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011
    """

    print(model1.summary())
    print(model2.summary())
    print(model3.summary())

    # AIC, BICê°€ ê°€ì¥ ë‚®ì€ ëª¨ë¸ì´ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ë¡œ íŒë‹¨ í•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì´ ëœë‹¤.

    print("\n", "=" * 3, "03.", "=" * 3)

    # ë°ì´í„°ì˜ ì˜¤ë¥˜ë¥¼ ê²€ì¦í•´ë³´ì
    # ë¯¸ë””ì–´ë³„ ë§¤ì²´ë¹„ ë¶„í¬ë¥¼ seabornì˜ distplotìœ¼ë¡œ ì‹œê°í™”\

    ax_temp = axes[1, 0]
    ax_temp.set_title('dist TV')
    sns.distplot(df['TV'], ax=ax_temp)

    ax_temp = axes[1, 1]
    ax_temp.set_title('dist radio')
    sns.distplot(df['radio'], ax=ax_temp)

    ax_temp = axes[1, 2]
    ax_temp.set_title('dist newspaper')
    sns.distplot(df['newspaper'], ax=ax_temp)


    df['log_newspaper'] = np.log(df['newspaper'] + 1)
    ax_temp = axes[1, 3]
    ax_temp.set_title('dist log_newspaper')
    sns.distplot(df['log_newspaper'], ax=ax_temp)
    print(df[['log_newspaper', 'newspaper']])

    # newspaper ê°’ì´ ì¹˜ìš°ì³ì ¸ ìˆìœ¼ë‹ˆ ì •ê·œí™”ë¥¼ ìœ„í•´ ë¡œê·¸ ë³€í™˜
    # 0ì´ ë˜ë©´ ìŒì˜ ë¬´í•œëŒ€ê°’ì´ ë˜ê¸°ë•Œë¬¸ì— + ìˆ«ìë¥¼ í•´ì¤˜ì„œ ë°©ì§€í•˜ì—¬ ë¡œê·¸ë¡œ ë³€ê²½í•œë‹¤.

    model1 = sm.ols(formula='sales ~ TV + radio + newspaper', data=df).fit()
    # model2 = sm.ols(formula='sales ~ TV + radio', data=df).fit()
    # model3 = sm.ols(formula='sales ~ TV', data=df).fit()
    model4 = sm.ols(formula='sales ~ TV + radio + log_newspaper', data=df).fit()

    print(model1.summary())
    print(model4.summary())

    plt.show()


    # ì ìš© ë°©ì•ˆ


    print(df.shape)


# marketing_01()


def marketing_02():
    """
        subject
            Machine_Running
        topic
            ex4. marketing_data
        content
            02. Retention
        Describe
            í—ˆìƒì  ì§€í‘œ(Vanity Metric)
            í–‰ë™ì  ì§€í‘œ (Actionable Metric)

            OMTM - facebook,
            ì„œë¹„ìŠ¤ì— ë³¸ì§ˆì— ê°€ê¹Œìš´ ë¶„ì„ì´ í•„ìš”í•¨

            STEP 1. ëª¨ë°”ì¼ ê²Œì„ A/B Test ë°ì´í„°
            STEP 2. ë‘ ì§‘ë‹¨ì˜ A/B Test Retention ë¹„êµ
            STEP 3. ë¶„ì„ ê²°ê³¼ í•´ì„ ì ìš© ë°©ì•ˆ

        DATA_SET
            userid - ê°œë³„ ìœ ì €ë“¤ì„ êµ¬ë¶„í•˜ëŠ” ì‹ë³„ ë²ˆí˜¸
            version - ìœ ì €ë“¤ì´ ì‹¤í—˜êµ° ëŒ€ì¡°êµ° ì¤‘ ì†í•œ ìœ„ì¹˜
            sum_gamerounds - ì²« ì„¤ì¹˜ í›„ 14ì¼ ê°„ ìœ ì €ê°€ í”Œë ˆì´í•œ ë¼ìš´ë“œì˜ ìˆ˜
            retention_1 - ìœ ì €ê°€ ì„¤ì¹˜ í›„ 1ì¼ ì´ë‚´ì— ë‹¤ì‹œ ëŒì•„ì™”ëŠ”ì§€ ì—¬ë¶€
            retention_7 - ìœ ì €ê°€ ì„¤ì¹˜ í›„ 7ì¼ ì´ë‚´ì— ë‹¤ì‹œ ëŒì•„ì™”ëŠ”ì§€ ì—¬ë¶€

        ub Contents
            01.
    """

    print("\n", "=" * 5, "02", "=" * 5)
    df = pd.read_csv("./data_file/cookie_cats.csv")
    """
        ê¸°ë³¸ ì²´í¬ ì‘ì—…
        1. ë°ì´í„° í™•ì¸   : df.shape, df.head(), df.tail()
        2. ê²°ì¸¡ê°’ ì¸¡ì •   : df.info(), df.isnull().sum()
        3. ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        4. ê¸°ìˆ í†µê³„ í™•ì¸ : df.describe()
        5. ë³€ìˆ˜ê°„ì˜ correlation í™•ì¸
        6. ë³€ìˆ˜ê°„ì˜ pairplot í™•ì¸
        7. Label, Feature(ì¸í’‹ ë³€ìˆ˜, ë…ë¦½ ë³€ìˆ˜) ì§€ì •
    """

    fig, axes = plt.subplots(2, 4, sharey=False, tight_layout=True, figsize=(15, 6), num='mobile game')

    # ë°ì´í„° í™•ì¸
    print(df.shape)
    print(df.tail())
    # ê²°ì¸¡ê°’ ì¸¡ì •
    print(df.info())
    print(df.isnull().sum())

    # ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    df = df[['userid', 'version', 'sum_gamerounds', 'retention_1', 'retention_7']]

    # ê¸°ìˆ í†µê³„ í™•ì¸ : df.describe()
    print(df.describe())

    # ë³€ìˆ˜ê°„ì˜ correlation í™•ì¸ df.corr() ì‹œê°í™”
    corr = df.corr()
    print(corr)
    # annot=True ìˆ«ì í‘œì‹œí•´ì¤Œ

    ax_temp = axes[0, 0]
    ax_temp.set_title('HeatMap')
    sns.heatmap(corr, annot=True, ax=ax_temp)

    ax_temp = axes[0, 1]
    ax_temp.set_title('sum_gamerounds box plot')
    sns.boxenplot(data=df, y='sum_gamerounds', ax=ax_temp)

    # ì´ìƒí•œ ë°ì´í„° ì œê±°
    df[df['sum_gamerounds'] > 45000]
    df = df[df['sum_gamerounds'] < 45000]
    print(df['sum_gamerounds'].describe())

    ax_temp = axes[0, 2]
    ax_temp.set_title('sum_gamerounds box plot')
    sns.boxenplot(data=df, y='sum_gamerounds', ax=ax_temp)

    print("\n", "=" * 3, "01.", "=" * 3)
    # ë°ì´í„° ë¶„ì„ì‹œì‘.
    # ê·¸ë£¹ í™•ì¸.
    print(df.groupby('version').count())

    # ê²Œì„ íšŸìˆ˜ë³„ ìœ ì €ìˆ˜ í™•ì¸
    plot_df = df.groupby('sum_gamerounds')['userid'].count()
    print(plot_df)

    ax_temp = axes[0, 3]
    ax_temp.set_title('Line plot')
    ax_temp.set_ylabel('Number of Player')
    ax_temp.set_xlabel('# Game rounds')
    plot_df[:300].plot(figsize=(10, 6), ax=ax_temp)

    ax_temp = axes[1, 0]
    ax_temp.set_title('distplot')
    sns.distplot(df['sum_gamerounds'], ax=ax_temp)

    # ax_temp = axes[1, 1]
    # ax_temp.set_title('Scatter Radio')
    # sns.scatterplot(data=df, x='retention_1', ax=ax_temp)
    #
    # ax_temp = axes[1, 2]
    # ax_temp.set_title('Scatter News')
    # sns.scatterplot(data=df, x='retention_7', ax=ax_temp)

    # 6. ë³€ìˆ˜ê°„ì˜ pairplot ì¶œë ¥
    # sns.pairplot(df[['TV', 'radio', 'newspaper', 'sales']])
    #
    # 7. Label, Feature(ì¸í’‹ ë³€ìˆ˜, ë…ë¦½ ë³€ìˆ˜) ì§€ì •
    # Labels = df['sales']
    # features = df[['TV', 'radio', 'newspaper']]
    # print(Labels.shape)
    # print(features.shape)


    # 1-day retention ì •ë³´ ì¡°íšŒ
    # í‰ê· 
    print('', df['retention_1'].mean())
    print('retention_1 í‰ê· ', df['retention_1'].mean(), df.groupby('version')['retention_1'].mean())
    print('retention_7 í‰ê· ', df['retention_7'].mean(), df.groupby('version')['retention_7'].mean())

    # ë¶„ì„ ê²°ê³¼ í•´ì„ ì ìš© ë°©ì•ˆ
    # ì •ë§ ë°ì´í„°ê°„ì˜ ì°¨ì´ê°€ ìˆëŠ”ê²ƒì¼ê¹Œ?

    # Bootstrap ë°©ë²• .
    # T-test ë°©ë²• (ì—°ì†í˜• ìˆ«ìì¼ë•Œ ê°€ëŠ¥)
    # Chai Square ì •ë§ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ ì°¨ì´ê°€ ìˆëŠ”ê²ƒì¸ê°€ ?

    boot_1d = []
    for i in range(1000):
        boot_mean = df.sample(frac=1, replace=True).groupby('version')['retention_1'].mean()
        boot_1d.append(boot_mean)

    # listë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    boot_1d = pd.DataFrame(boot_1d)

    # A Kernel Density Estimate plot of the bootstrap distributions
    boot_1d.plot(kind='density')
    plt.show()

    """
        ìœ„ì˜ ë‘ ë¶„í¬ëŠ” AB ë‘ ê·¸ë£¹ì— ëŒ€í•´ 1 day retentionì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ë¶€íŠ¸ ìŠ¤íŠ¸ë© ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•©ë‹ˆë‹¤.
        ë¹„ë¡ ì‘ì§€ë§Œ ì°¨ì´ì˜ ì¦ê±°ê°€ìˆëŠ” ê²ƒ ê°™ì•„ ë³´ì…ë‹ˆë‹¤.
        ìì„¸íˆ ì‚´í´ë³´ê¸° ìœ„í•´ % ì°¨ì´ë¥¼ ê·¸ë ¤ ë´…ì‹œë‹¤.
    """

    # ë‘ AB ê·¸ë£¹ê°„ì˜ % ì°¨ì´ í‰ê·  ì»¬ëŸ¼ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    boot_1d['diff'] = (boot_1d.gate_30 - boot_1d.gate_40) / boot_1d.gate_40 * 100

    # bootstrap % ì°¨ì´ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤.
    ax = boot_1d['diff'].plot(kind='density')
    ax.set_title('% difference in 1-day retention between the two AB-groups')

    # ê²Œì´íŠ¸ê°€ ë ˆë²¨30ì— ìˆì„ ë•Œ 1-day retentionì´ í´ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    print('ê²Œì´íŠ¸ê°€ ë ˆë²¨30ì— ìˆì„ ë•Œ 1-day retentionì´ í´ í™•ë¥ :', (boot_1d['diff'] > 0).mean())
    """
        ìœ„ ë„í‘œì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ % ì°¨ì´ëŠ” ì•½ 1%-2%ì´ë©° ë¶„í¬ì˜ 95%ëŠ” 0% ì´ìƒì´ë©° ë ˆë²¨ 30ì˜ ê²Œì´íŠ¸ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤.
        ë¶€íŠ¸ ìŠ¤íŠ¸ë© ë¶„ì„ì— ë”°ë¥´ë©´ ê²Œì´íŠ¸ê°€ ë ˆë²¨ 30ì—ìˆì„ ë•Œ 1ì¼ ìœ ì§€ìœ¨ì´ ë” ë†’ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
        ê·¸ëŸ¬ë‚˜ í”Œë ˆì´ì–´ëŠ” í•˜ë£¨ ë™ì•ˆ ë§Œ ê²Œì„ì„í–ˆê¸° ë•Œë¬¸ì— ëŒ€ë¶€ë¶„ì˜ í”Œë ˆì´ì–´ê°€ ì•„ì§ ë ˆë²¨ 30ì— ë‹¤ë‹¤ë¥´ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.
        ì¦‰, ëŒ€ë¶€ë¶„ì˜ ìœ ì €ë“¤ì€ ê²Œì´íŠ¸ê°€ 30ì— ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ retentionì´ ì˜í–¥ë°›ì§€ ì•Šì•˜ì„ ê²ƒì…ë‹ˆë‹¤.
        ì¼ì£¼ì¼ ë™ì•ˆ í”Œë ˆì´ í•œ í›„ì—ëŠ” ë” ë§ì€ í”Œë ˆì´ì–´ê°€ ë ˆë²¨ 30ê³¼ 40ì— ë„ë‹¬í•˜ê¸° ë•Œë¬¸ì— 7 ì¼ retentionë„ í™•ì¸í•´ì•¼í•©ë‹ˆë‹¤.
    """

    df.groupby('version')['retention_7'].sum() / df.groupby('version')['retention_7'].count()

    boot_7d = []
    for i in range(500):
        boot_mean = df.sample(frac=1, replace=True).groupby('version')['retention_7'].mean()
        boot_7d.append(boot_mean)

    # listë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    boot_7d = pd.DataFrame(boot_7d)

    # ë‘ AB ê·¸ë£¹ê°„ì˜ % ì°¨ì´ í‰ê·  ì»¬ëŸ¼ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    boot_7d['diff'] = (boot_7d.gate_30 - boot_7d.gate_40) / boot_7d.gate_40 * 100

    # bootstrap % ì°¨ì´ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤.
    ax = boot_7d['diff'].plot(kind='density')
    ax.set_title('% difference in 7-day retention between the two AB-groups')

    # ê²Œì´íŠ¸ê°€ ë ˆë²¨30ì— ìˆì„ ë•Œ 7-day retentionì´ ë” í´ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    print('ê²Œì´íŠ¸ê°€ ë ˆë²¨30ì— ìˆì„ ë•Œ 7-day retentionì´ í´ í™•ë¥ :', (boot_7d['diff'] > 0).mean())

    plt.show()

    print("\n", "=" * 3, "02.", "=" * 3)
    # T-test í†µê³„ì ì¸ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ë°©ë²•.
    """
        https://www.statisticshowto.com/probability-and-statistics/t-test/
        T Score
            t-scoreê°€ í¬ë©´ ë‘ ê·¸ë£¹ì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            t-scoreê°€ ì‘ìœ¼ë©´ ë‘ ê·¸ë£¹ì´ ë¹„ìŠ·í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
        P-values
            p-valueëŠ” 5%ìˆ˜ì¤€ì—ì„œ 0.05ì…ë‹ˆë‹¤.
            p-valuesëŠ” ì‘ì€ ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì´ê²ƒì€ ë°ì´í„°ê°€ ìš°ì—°íˆ ë°œìƒí•œ ê²ƒì´ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            ì˜ˆë¥¼ ë“¤ì–´ p-valueê°€ 0.01 ì´ë¼ëŠ” ê²ƒì€ ê²°ê³¼ê°€ ìš°ì—°íˆ ë‚˜ì˜¬ í™•ë¥ ì´ 1%ì— ë¶ˆê³¼í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            ëŒ€ë¶€ë¶„ì˜ ê²½ìš° 0.05 (5%) ìˆ˜ì¤€ì˜ p-valueë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìŠµë‹ˆë‹¤. ì´ ê²½ìš° í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ë‹¤ê³  í•©ë‹ˆë‹¤.
            
        ìœ„ ë¶„ì„ê²°ê³¼ë¥¼ ë³´ë©´, ë‘ ê·¸ë£¹ì—ì„œ retention_1ì— ìˆì–´ì„œëŠ” ìœ ì˜í•˜ì§€ ì•Šê³ , retention_7ì—ì„œëŠ” ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ë‹¤ì‹œë§í•´, retention_7ì´ gate30ì´ gate40 ë³´ë‹¤ ë†’ì€ ê²ƒì€ ìš°ì—°íˆ ë°œìƒí•œ ì¼ì´ ì•„ë‹™ë‹ˆë‹¤.
        ì¦‰, gateëŠ” 30ì— ìˆëŠ” ê²ƒì´ 40ì— ìˆëŠ” ê²ƒë³´ë‹¤ retention 7 ì°¨ì›ì—ì„œ ë” ì¢‹ì€ ì„ íƒì§€ ì…ë‹ˆë‹¤.
    """


    # ìˆ˜í•™ì  ê³„ì‚°ì„ í•´ì£¼ëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€
    from scipy import stats

    df_30 = df[df['version'] == 'gate_30']
    df_40 = df[df['version'] == 'gate_40']

    # ë…ë¦½í‘œë³¸ T-ê²€ì • (2 Sample T-Test)
    tTestResult = stats.ttest_ind(df_30['retention_1'], df_40['retention_1'])
    tTestResultDiffVar = stats.ttest_ind(df_30['retention_1'], df_40['retention_1'], equal_var=False)

    print(tTestResult, tTestResultDiffVar)

    tTestResult = stats.ttest_ind(df_30['retention_7'], df_40['retention_7'])
    tTestResultDiffVar = stats.ttest_ind(df_30['retention_7'], df_40['retention_7'], equal_var=False)
    print(tTestResult, tTestResultDiffVar)

    print("\n", "=" * 3, "03.", "=" * 3)

    # chi-square
    """
        chi-square
        ì‚¬ì‹¤ t-testëŠ” retention ì—¬ë¶€ë¥¼ 0,1 ë¡œ ë‘ê³  ë¶„ì„í•œ ê²ƒì…ë‹ˆë‹¤.
        í•˜ì§€ë§Œ ì‹¤ì œë¡œ retention ì—¬ë¶€ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ ë°©ë²•ë³´ë‹¤ëŠ” chi-squareê²€ì •ì„ í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.
        ì¹´ì´ì œê³±ê²€ì •ì€ ì–´ë–¤ ë²”ì£¼í˜• í™•ë¥ ë³€ìˆ˜ ğ‘‹ ê°€ ë‹¤ë¥¸ ë²”ì£¼í˜• í™•ë¥ ë³€ìˆ˜ ğ‘Œ ì™€ ë…ë¦½ì¸ì§€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ”ê°€ë¥¼ ê²€ì¦í•˜ëŠ”ë°ë„ ì‚¬ìš©ë©ë‹ˆë‹¤.
        ì¹´ì´ì œê³±ê²€ì •ì„ ë…ë¦½ì„ í™•ì¸í•˜ëŠ”ë° ì‚¬ìš©í•˜ë©´ ì¹´ì´ì œê³± ë…ë¦½ê²€ì •ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.
        ë§Œì•½ ë‘ í™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì´ë¼ë©´ ğ‘‹=0 ì¼ ë•Œì˜ ğ‘Œ ë¶„í¬ì™€ ğ‘‹=1 ì¼ ë•Œì˜ ğ‘Œ ë¶„í¬ê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.
        ë‹¤ì‹œë§í•´ ë²„ì „ì´ 30ì¼ë•Œì™€ 40ì¼ ë•Œ ëª¨ë‘ Yì˜ ë¶„í¬ê°€ ê°™ì€ ê²ƒì…ë‹ˆë‹¤.
        ë”°ë¼ì„œ í‘œë³¸ ì§‘í•©ì´ ê°™ì€ í™•ë¥ ë¶„í¬ì—ì„œ ë‚˜ì™”ë‹¤ëŠ” ê²ƒì„ ê·€ë¬´ê°€ì„¤ë¡œ í•˜ëŠ” ì¹´ì´ì œê³±ê²€ì •ì„ í•˜ì—¬ ì±„íƒëœë‹¤ë©´ ë‘ í™•ë¥ ë³€ìˆ˜ëŠ” ë…ë¦½ì…ë‹ˆë‹¤.
        ë§Œì•½ ê¸°ê°ëœë‹¤ë©´ ë‘ í™•ë¥ ë³€ìˆ˜ëŠ” ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.
        ë‹¤ì‹œë§í•´ ì¹´ì´ì œê³±ê²€ì • ê²°ê³¼ê°€ ê¸°ê°ëœë‹¤ë©´ ê²Œì´íŠ¸ê°€ 30ì¸ì§€ 40ì¸ì§€ ì—¬ë¶€ì— ë”°ë¼ retentionì˜ ê°’ì´ ë³€í™”í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
        ğ‘‹ ì˜ ê°’ì— ë”°ë¥¸ ê°ê°ì˜ ğ‘Œ ë¶„í¬ê°€ 2ì°¨ì› í‘œ(contingency table)ì˜ í˜•íƒœë¡œ ì£¼ì–´ì§€ë©´ ë…ë¦½ì¸ ê²½ìš°ì˜ ë¶„í¬ì™€ ì‹¤ì œ y í‘œë³¸ë³¸í¬ì˜ ì°¨ì´ë¥¼ ê²€ì •í†µê³„ëŸ‰ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì´ ê°’ì´ ì¶©ë¶„íˆ í¬ë‹¤ë©´ ğ‘‹ ì™€ ğ‘Œ ëŠ” ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.
    """
    # ë¶„í• í‘œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ë²„ì „ë³„ë¡œ ìƒì¡´ìì˜ ìˆ˜ í•©ê³„ë¥¼ êµ¬í•©ë‹ˆë‹¤.
    df.groupby('version').sum()

    # ë²„ì „ë³„ ì „ì²´ ìœ ì €ì˜ ìˆ˜ë¥¼ êµ¬í•©ë‹ˆë‹¤.
    df.groupby('version').count()

    import scipy as sp
    gate_40_01_sum = df.groupby('version').sum()['retention_1']['gate_40']
    gate_30_01_sum = df.groupby('version').sum()['retention_1']['gate_30']
    gate_40_07_sum = df.groupby('version').sum()['retention_7']['gate_40']
    gate_30_07_sum = df.groupby('version').sum()['retention_7']['gate_30']

    gate_40_01_cnt = df.groupby('version').count()['retention_1']['gate_40']
    gate_30_01_cnt = df.groupby('version').count()['retention_1']['gate_30']
    gate_40_07_cnt = df.groupby('version').count()['retention_7']['gate_40']
    gate_30_07_cnt = df.groupby('version').count()['retention_7']['gate_30']


    obs1 = np.array([[gate_40_01_sum, (gate_40_01_cnt - gate_40_01_sum)], [gate_30_01_sum, (gate_30_01_cnt - gate_30_01_sum)]])
    sp.stats.chi2_contingency(obs1)

    obs2 = np.array([[gate_40_07_sum, (gate_40_07_cnt - gate_40_07_sum)], [gate_30_07_sum, (gate_30_07_cnt - gate_30_07_sum)]])
    sp.stats.chi2_contingency(obs2)

    """
        OBS-1
        P-valueê°€ ì¤‘ìš” (0.075)
        (3.1698355431707994,
         0.07500999897705699,
         1,
         array([[20252.35970417, 25236.64029583],
                [19900.64029583, 24798.35970417]]))
        OBS-2
        P-valueê°€ ì¤‘ìš” (0.001)
        (9.915275528905669,
         0.0016391259678654423,
         1,
         array([[ 8463.49203885, 37025.50796115],
                [ 8316.50796115, 36382.49203885]]))
        ì¹´ì´ì œê³± ë…ë¦½ê²€ì •ì˜ ìœ ì˜í™•ë¥ ì€ 0.1%ì…ë‹ˆë‹¤.
        ì¦‰ ğ‘‹ ì™€ ğ‘Œ ëŠ” ìƒê´€ê´€ê³„ê°€ ìˆë‹¤ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ê²Œì´íŠ¸ê°€ 30ì— ìˆëŠ”ì§€ 40ì— ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ 7ì¼ ë’¤ retentionì´ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.
        7ì¼ ë’¤ retention ìœ ì§€ë¥¼ ìœ„í•˜ì—¬ ê²Œì´íŠ¸ëŠ” 30ì— ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
    """

    """
        Bootstrap 
        T-Test  : í¬ê³  ì‘ì€ê²ƒì— ì˜ë¯¸ê°€ ìˆëŠ” ë°ì´í„°ì¼ ê²½ìš° ..? 
        Chai Square ë°ì´í„°ê°€ True Falseë¡œ ë‚˜ì˜¬ë•Œ..
        
        ê²°ë¡ 
            gateëŠ” 30ì— ìœ ì§€í•´ì•¼í•©ë‹ˆë‹¤.
            
            ë” ìƒê°í•´ ë³¼ ê²ƒ
            ì‹¤ì œë¡œëŠ” retention ì´ì™¸ì— í•¨ê»˜ ê³ ë ¤í•´ì•¼ í•  ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ë“¤ì´ ìˆìŠµë‹ˆë‹¤.
            ì•±ë‚´ êµ¬ë§¤, ê²Œì„ í”Œë ˆì´ íšŸìˆ˜, ì¹œêµ¬ì´ˆëŒ€ë¡œ ì¸í•œ referrer ë“± ì…ë‹ˆë‹¤.
            ë³¸ ë°ì´í„°ì—ì„œëŠ” retentionë§Œ ì£¼ì–´ì ¸ ìˆê¸°ì— í•œ ê°€ì§€ë¥¼ ì£¼ì•ˆì ì„ ë‘ì–´ ë¶„ì„ í–ˆìŠµë‹ˆë‹¤.
            ì„œë¹„ìŠ¤ ìš´ì˜ì, ê¸°íšì ì°¨ì›ì—ì„œ ì •ë§ ì¤‘ìš”í•œ ë©”íŠ¸ë¦­ì„ ì •í•˜ê³  ê·¸ ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    """


# marketing_02()


def marketing_03():
    """
        subject
            Machine_Running
        topic
            ex4. marketing_data
        content
            03. Revenue ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë‚˜ëˆ ë³´ì
        Describe
            ë¶„ì„í•  ë°ì´í„° íŒŒì•…
            mall ë°ì´í„° EDA
                CustomerID - ê³ ê°ë“¤ì—ê²Œ ë°°ì •ëœ ìœ ë‹ˆí¬í•œ ê³ ê° ë²ˆí˜¸ ì…ë‹ˆë‹¤.
                Gender - ê³ ê°ì˜ ì„±ë³„ ì…ë‹ˆë‹¤.
                Age - ê³ ê°ì˜ ë‚˜ì´ ì…ë‹ˆë‹¤.
                Annual Income (k$) - ê³ ê°ì˜ ì—°ì†Œë“ ì…ë‹ˆë‹¤.
                Spending Score (1-100) - ê³ ê°ì˜ êµ¬ë§¤í–‰ìœ„ì™€ êµ¬ë§¤ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ mallì—ì„œ í• ë‹¹í•œ ê³ ê°ì˜ ì§€ë¶ˆ ì ìˆ˜ ì…ë‹ˆë‹¤.

            ë¬¸ì œ ì •ì˜
                ì „ì œ
                    ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì ì ˆ ì •í™•í•˜ê²Œ ìˆ˜ì§‘, ê³„ì‚°ëœ ê²ƒì¸ì§€ì— ëŒ€í•œ ê²€ì¦ë¶€í„° ì‹œì‘í•´ì•¼í•˜ì§€ë§Œ,
                    ì§€ê¸ˆì€ ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì •í™•í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
                    (ì˜ˆ: Spending ScoreëŠ” ì ì ˆí•˜ê²Œ ì‚°ì¶œëœ ê²ƒì´ë¼ í™•ì‹ í•˜ê³  ì‹œì‘í•©ë‹ˆë‹¤)
                    ì£¼ì–´ì§„ ë³€ìˆ˜ë“¤ì„ ê°€ì§€ê³  ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
                    ê°€ì¥ ì ì ˆí•œ ìˆ˜ì˜ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
                ë¶„ì„ì˜ ëª©ì 
                    ê° ì„¸ê·¸ë¨¼íŠ¸ ë³„ íŠ¹ì„±ì„ ë„ì¶œí•©ë‹ˆë‹¤.
                    ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ íŠ¹ì„±ì— ë§ëŠ” í™œìš©ë°©ì•ˆ, ì „ëµì„ ê³ ë¯¼í•´ë´…ë‹ˆë‹¤.
        sub Contents
            01.
    """

    df = pd.read_csv("./data_file/Mall_Customers.csv")
    fig, axes = plt.subplots(2, 4, sharey=False, tight_layout=True, figsize=(15, 6), num='mobile game')

    # ë°ì´í„° í™•ì¸
    print(df.shape)
    print(df.tail())

    # ê²°ì¸¡ê°’ ì¸¡ì •
    print(df.info())
    print(df.isnull().sum())

    # ê¸°ìˆ í†µê³„ í™•ì¸ : df.describe()
    print(df.describe())

    # ë³€ìˆ˜ê°„ì˜ correlation í™•ì¸ df.corr() ì‹œê°í™”
    def open_data_graph():
        corr = df.corr()
        print(corr)

        ax_temp = axes[0, 0]
        ax_temp.set_title('HeatMap')
        sns.heatmap(corr, annot=True, ax=ax_temp)

        # pairplot ì‹œê°í™” ìƒì„±
        print(df.columns)

        ax_temp = axes[0, 1]
        ax_temp.set_title('Age dist plot')
        sns.distplot(df['Age'], ax=ax_temp)

        ax_temp = axes[0, 2]
        ax_temp.set_title('Annual Income (k$) dist plot')
        sns.distplot(df['Age'], ax=ax_temp)

        ax_temp = axes[0, 3]
        ax_temp.set_title('Spending Score (1-100) dist plot')
        sns.distplot(df['Spending Score (1-100)'], ax=ax_temp)

        ax_temp = axes[1, 0]
        ax_temp.set_title('Genter Count plot')
        sns.countplot(df['Gender'], ax=ax_temp)

        ax_temp = axes[1, 1]
        ax_temp.set_title('boxplot plot')
        sns.boxplot(data=df, x='Gender', y='Age', hue='Gender', palette=['m', 'g'], ax=ax_temp)


        # ì„œë¸Œí”Œë¡¯ì— ì•ˆë“¤ì–´ê°€ëŠ” ê²ƒë“¤
        sns.pairplot(df[['CustomerID', 'Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']])
        sns.pairplot(df[['CustomerID', 'Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']], hue='Gender')
        # lmplotì€ regplotì„ ì„œë¸Œ í”Œë¡¯ìœ¼ë¡œ í•˜ëŠ” í”Œë¡¯ì…ë‹ˆë‹¤
        sns.lmplot(data=df, x='Age', y='Annual Income (k$)', hue='Gender', fit_reg=False)
        sns.lmplot(data=df, x='Spending Score (1-100)', y='Annual Income (k$)', hue='Gender', fit_reg=False)

        plt.show()

    # open_data_graph()


    print("\n", "=" * 5, "03", "=" * 5)

    print("\n", "=" * 3, "01.", "=" * 3)
    """
        ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ í´ëŸ¬ìŠ¤í„°ë§
        K-meansë¥¼ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§
            K-meansëŠ” ê°€ì¥ ë¹ ë¥´ê³  ë‹¨ìˆœí•œ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ì¤‘ í•œ ê°€ì§€ ì…ë‹ˆë‹¤.
            scikit-learnì˜ cluster ì„œë¸ŒíŒ¨í‚¤ì§€ KMeans í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
        
            n_clusters: êµ°ì§‘ì˜ ê°¯ìˆ˜ (default=8)
            init: ì´ˆê¸°í™” ë°©ë²•. "random"ì´ë©´ ë¬´ì‘ìœ„, "k-means++"ì´ë©´ K-í‰ê· ++ ë°©ë²•.(default=k-means++)
            n_init: centroid seed ì‹œë„ íšŸìˆ˜. ë¬´ì‘ìœ„ ì¤‘ì‹¬ìœ„ì¹˜ ëª©ë¡ ì¤‘ ê°€ì¥ ì¢‹ì€ ê°’ì„ ì„ íƒí•œë‹¤.(default=10)
            max_iter: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜.(default=300)
            random_state: ì‹œë“œê°’.(default=None)     
        
        2ê°€ì§€ ë³€ìˆ˜ê°€ ì•„ë‹Œ ì—¬ëŸ¬ê°€ì§€ ë³€ìˆ˜ë¥¼ í™œìš©í•œí›„ ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ì„œ êµ°ì§‘í™” í•œë‹¤.
    """
    ### Age & spending Score ë‘ ê°€ì§€ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§
    # ['CustomerID', 'Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']


    from sklearn.cluster import KMeans
    # X1ì— 'Age' , 'Spending Score (1-100)'ì˜ ê°’ì„ ë„£ì–´ì¤ë‹ˆë‹¤.
    x1 = df[['Age', 'Spending Score (1-100)']].values
    print(x1.shape)

    # inertia ë¼ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
    inertia = []

    # êµ°ì§‘ìˆ˜ nì„ 1ì—ì„œ 20ê¹Œì§€ ëŒì•„ê°€ë©° X1ì— ëŒ€í•´ k-means++ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ inertiaë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.
    for n in range(1, 20):
        algorithm = (KMeans(n_clusters=n, random_state=30))
        algorithm.fit(x1)
        inertia.append(algorithm.inertia_)

    """
        Inertia valueë¥¼ ì´ìš©í•œ ì ì • k ì„ íƒ
        ê´€ì„±(Inertia)ì— ê¸°ë°˜í•˜ì—¬ n ê°œìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        ê´€ì„±(Inertia) : ê° ì¤‘ì‹¬ì (centroid)ì—ì„œ êµ°ì§‘ ë‚´ ë°ì´í„°ê°„ì˜ ê±°ë¦¬ë¥¼ í•©ì‚°í•œ ê²ƒìœ¼ë¡œ êµ°ì§‘ì˜ ì‘ì§‘ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 
        ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ ì‘ì§‘ë„ ë†’ì€ êµ°ì§‘í™” ì…ë‹ˆë‹¤. ì¦‰, ì‘ì„ ìˆ˜ë¡ ì¢‹ì€ ê°’ ì…ë‹ˆë‹¤.
        https://scikit-learn.org/stable/modules/clustering.html
    """


    print(inertia)
    axes[0, 0].plot(np.arange(1, 20), inertia, 'o')
    axes[0, 0].plot(np.arange(1, 20), inertia, '-', alpha=0.8)
    axes[0, 0].set_title('KMeans small is best')
    axes[0, 0].set_xlabel('Number of Clusters')
    axes[0,0].set_ylabel('Inertia')
    plt.show()

    # ê¸‰ê²©í•˜ê²Œ ë³€í•˜ëŠ” ì§€ì ì´ êµ°ì§‘ìœ¼ë¡œ ì¡ê¸°ì— ì¢‹ë‹¤.
    # êµ°ì§‘ìˆ˜ë¥¼ 4ë¡œ ì§€ì •í•˜ì—¬ ì‹œê°í™” í•´ë´…ë‹ˆë‹¤.
    algorithm = (KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300,
                        tol=0.0001, random_state=111, algorithm='elkan'))
    algorithm.fit(x1)
    labels1 = algorithm.labels_
    centroids1 = algorithm.cluster_centers_

    h = 0.02
    x_min, x_max = x1[:, 0].min() - 1, x1[:, 0].max() + 1
    y_min, y_max = x1[:, 1].min() - 1, x1[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])

    plt.figure(1, figsize=(15, 7))
    plt.clf()
    Z = Z.reshape(xx.shape)
    plt.imshow(Z, interpolation='nearest',
               extent=(xx.min(), xx.max(), yy.min(), yy.max()),
               cmap=plt.cm.Pastel2, aspect='auto', origin='lower')

    plt.scatter(x='Age', y='Spending Score (1-100)', data=df, c=labels1,
                s=200)
    plt.scatter(x=centroids1[:, 0], y=centroids1[:, 1], s=300, c='red', alpha=0.5)
    plt.ylabel('Spending Score (1-100)'), plt.xlabel('Age')
    plt.show()

    """
        ì—°ë ¹-ì†Œë¹„ì ìˆ˜ë¥¼ í™œìš©í•œ êµ°ì§‘ 4ê°œëŠ” ì•„ë˜ì™€ ê°™ì´ ëª…ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        ì €ì—°ë ¹-ê³ ì†Œë¹„ êµ°
        ì €ì—°ë ¹-ì¤‘ì†Œë¹„ êµ°
        ê³ ì—°ë ¹-ì¤‘ì†Œë¹„ êµ°
        ì €ì†Œë¹„ êµ°
        êµ°ì§‘ë³„ í™œìš© ì „ëµ ì˜ˆì‹œ
            ì´ ìˆ˜í¼ë§ˆì¼“ mallì˜ ê²½ìš° ì†Œë¹„ì ìˆ˜ê°€ ë†’ì€ ê³ ê°ë“¤ì€ ëª¨ë‘ 40ì„¸ ì´í•˜ì˜ ì Šì€ ê³ ê°ì…ë‹ˆë‹¤.
            ì†Œë¹„ì ìˆ˜ê°€ ë†’ì€ ê³ ê°ë“¤ì€ ì—°ë ¹ëŒ€ê°€ ë¹„ìŠ·í•œ ë§Œí¼ ë¹„ìŠ·í•œ êµ¬ë§¤íŒ¨í„´ê³¼ ì·¨í–¥ì„ ê°€ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
            í•´ë‹¹ êµ°ì§‘ì˜ ì†Œë¹„ì íŠ¹ì„±ì„ ë” ë¶„ì„í•´ë³¸ ë’¤ í•´ë‹¹ êµ°ì§‘ì˜ ì†Œë¹„ì ëŒ€ìƒ VIP ì „ëµì„ ìˆ˜ë¦½í•´ë´…ë‹ˆë‹¤.
            ì†Œë¹„ì ìˆ˜ê°€ ì¤‘ê°„ì •ë„ì¸ ê³ ê°ë“¤ì—ê²ŒëŠ” ì—°ë ¹ì— ë”°ë¼ ë‘ ê°œ ì§‘ë‹¨ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì ‘ê·¼í•´ë´…ë‹ˆë‹¤.
            ì†Œë¹„ì ìˆ˜ê°€ ë‚®ì€ ê³ ê°êµ°ì€ ì—°ë ¹ëŒ€ë³„ë¡œ ì¤‘ì†Œë¹„ì ìˆ˜ êµ°ì§‘ì— í¸ì…ë  ìˆ˜ ìˆë„ë¡ ì ‘ê·¼í•´ë´…ë‹ˆë‹¤.
    """

    print("\n", "=" * 3, "02.", "=" * 3)
    # X1ì— 'Annual Income (k$)' , 'Spending Score (1-100)' ì˜ ê°’ì„ ë„£ì–´ì¤ë‹ˆë‹¤.
    X2 = df[['Annual Income (k$)', 'Spending Score (1-100)']].values

    # inertia ë¼ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
    inertia = []

    # êµ°ì§‘ìˆ˜ nì„ 1ì—ì„œ 11ê¹Œì§€ ëŒì•„ê°€ë©° X1ì— ëŒ€í•´ k-means++ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ inertiaë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.
    for n in range(1, 11):
        algorithm = (KMeans(n_clusters=n))
        algorithm.fit(X2)
        inertia.append(algorithm.inertia_)

    plt.figure(1, figsize=(16, 5))
    plt.plot(np.arange(1, 11), inertia, 'o')
    plt.plot(np.arange(1, 11), inertia, '-', alpha=0.8)
    plt.xlabel('Number of Clusters'), plt.ylabel('Inertia')
    plt.show()

    # êµ°ì§‘ìˆ˜ë¥¼ 5ë¡œ ì§€ì •í•˜ì—¬ ì‹œê°í™” í•´ë´…ë‹ˆë‹¤.
    algorithm = (KMeans(n_clusters=5, init='k-means++', n_init=10, max_iter=300,
                        tol=0.0001, random_state=111, algorithm='elkan'))
    algorithm.fit(X2)
    labels2 = algorithm.labels_
    centroids2 = algorithm.cluster_centers_

    h = 0.02
    x_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1
    y_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])

    plt.figure(1, figsize=(15, 7))
    plt.clf()
    Z2 = Z2.reshape(xx.shape)
    plt.imshow(Z2, interpolation='nearest',
               extent=(xx.min(), xx.max(), yy.min(), yy.max()),
               cmap=plt.cm.Pastel2, aspect='auto', origin='lower')

    plt.scatter(x='Annual Income (k$)', y='Spending Score (1-100)', data=df, c=labels2, s=200)
    plt.scatter(x=centroids2[:, 0], y=centroids2[:, 1], s=300, c='red', alpha=0.5)
    plt.ylabel('Spending Score (1-100)'), plt.xlabel('Annual Income (k$)')
    plt.show()

    print("\n", "=" * 3, "03.", "=" * 3)

    """
        ë¶„ì„ ëª¨ë¸ë§ / ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ í•´ì„.
        ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¥¼ ì‚¬ìš©í•œ k ì„ íƒ
        ê°€ì¥ ì¢‹ì€ ê°’ì€ 1ì´ê³  ìµœì•…ì˜ ê°’ì€ -1
       
            Silhouette CoefficientëŠ” ê° ìƒ˜í”Œì˜ í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ê±°ë¦¬ì˜ í‰ê·  (a)ì™€ ì¸ì ‘ í´ëŸ¬ìŠ¤í„°ì™€ì˜ ê±°ë¦¬ í‰ê·  (b)ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
            í•œ ìƒ˜í”Œì˜ Silhouette CoefficientëŠ” (b - a) / max(a, b)ì…ë‹ˆë‹¤.
            
            0 ê·¼ì²˜ì˜ ê°’ì€ í´ëŸ¬ìŠ¤í„°ê°€ ì˜¤ë²„ë©ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤
            ìŒìˆ˜ ê°’ì€ ìƒ˜í”Œì´ ì˜ëª»ëœ í´ëŸ¬ìŠ¤í„°ì— ë°°ì •ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ê°€ ë” ìœ ì‚¬í•œ êµ°ì§‘ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html
    """
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_samples, silhouette_score
    import matplotlib.cm as cm

    # í´ëŸ¬ìŠ¤í„°ì˜ ê°¯ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
    range_n_clusters = [6]

    # ì‚¬ìš©í•  ì»¬ëŸ¼ ê°’ì„ ì§€ì •í•´ì¤ë‹ˆë‹¤.
    X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].values

    for n_clusters in range_n_clusters:
        # 1 X 2 ì˜ ì„œë¸Œí”Œë¡¯ì„ ë§Œë“­ë‹ˆë‹¤.
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_size_inches(18, 7)

        # ì²« ë²ˆì§¸ ì„œë¸Œí”Œë¡¯ì€ ì‹¤ë£¨ì—£ í”Œë¡¯ì…ë‹ˆë‹¤.
        # silhouette coefficientëŠ” -1ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.
        # í•˜ì§€ë§Œ ì‹œê°í™”ì—ì„œëŠ” -0.1ì—ì„œ 1ì‚¬ì´ë¡œ ì§€ì •í•´ì¤ë‹ˆë‹¤.
        ax1.set_xlim([-0.1, 1])

        # clustererë¥¼ n_clusters ê°’ìœ¼ë¡œ ì´ˆê¸°í™” í•´ì¤ë‹ˆë‹¤.
        # ì¬í˜„ì„±ì„ ìœ„í•´ random seedë¥¼ 10ìœ¼ë¡œ ì§€ì • í•©ë‹ˆë‹¤.
        clusterer = KMeans(n_clusters=n_clusters, random_state=10)
        cluster_labels = clusterer.fit_predict(X)

        # silhouette_scoreëŠ” ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ í‰ê· ê°’ì„ ì œê³µí•©ë‹ˆë‹¤.
        # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ëŠ” í˜•ì„±ëœ êµ°ì§‘ì— ëŒ€í•´ ë°€ë„(density)ì™€ ë¶„ë¦¬(seperation)ì— ëŒ€í•´ ê²¬í•´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        silhouette_avg = silhouette_score(X, cluster_labels)
        print("For n_clusters =", n_clusters,
              "The average silhouette_score is :", silhouette_avg)

        # ê° ìƒ˜í”Œì— ëŒ€í•œ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        sample_silhouette_values = silhouette_samples(X, cluster_labels)

        y_lower = 10
        for i in range(n_clusters):
            # í´ëŸ¬ìŠ¤í„° iì— ì†í•œ ìƒ˜í”Œë“¤ì˜ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¥¼ ì·¨í•©í•˜ì—¬ ì •ë ¬í•©ë‹ˆë‹¤.
            ith_cluster_silhouette_values = \
                sample_silhouette_values[cluster_labels == i]

            ith_cluster_silhouette_values.sort()

            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

            color = cm.nipy_spectral(float(i) / n_clusters)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                              0, ith_cluster_silhouette_values,
                              facecolor=color, edgecolor=color, alpha=0.7)

            # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„ì„ ë‹¬ì•„ì„œ ì‹¤ë£¨ì—£ í”Œë¡¯ì˜ Labelì„ ì§€ì •í•´ì¤ë‹ˆë‹¤.
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

            # ë‹¤ìŒ í”Œë¡¯ì„ ìœ„í•œ ìƒˆë¡œìš´ y_lowerë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
            y_lower = y_upper + 10  # 10 for the 0 samples

        ax1.set_title("The silhouette plot for the various clusters.")
        ax1.set_xlabel("The silhouette coefficient values")
        ax1.set_ylabel("Cluster label")

        # ëª¨ë“  ê°’ì— ëŒ€í•œ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ì˜ í‰ê· ì„ ìˆ˜ì§ì„ ìœ¼ë¡œ ê·¸ë ¤ì¤ë‹ˆë‹¤.
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

        ax1.set_yticks([])  # yaxis labels / ticks ë¥¼ ì§€ì›Œì¤ë‹ˆë‹¤.
        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

        # ë‘ ë²ˆì§¸ í”Œë¡¯ì´ ì‹¤ì œ í´ëŸ¬ìŠ¤í„°ê°€ ì–´ë–»ê²Œ í˜•ì„±ë˜ì—ˆëŠ”ì§€ ì‹œê°í™” í•©ë‹ˆë‹¤.
        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                    c=colors, edgecolor='k')

        # í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„ì„ ì§€ì–´ì¤ë‹ˆë‹¤.
        centers = clusterer.cluster_centers_
        # í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì•™ì— í•˜ì–€ ë™ê·¸ë¼ë¯¸ë¥¼ ê·¸ë ¤ì¤ë‹ˆë‹¤.
        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                    c="white", alpha=1, s=200, edgecolor='k')

        for i, c in enumerate(centers):
            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                        s=50, edgecolor='k')

        ax2.set_title("The visualization of the clustered data.")
        ax2.set_xlabel("Feature space for the 1st feature")
        ax2.set_ylabel("Feature space for the 2nd feature")

        plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                      "with n_clusters = %d" % n_clusters),
                     fontsize=14, fontweight='bold')

    plt.show()

    print(n_clusters, cluster_labels, cluster_labels.shape)
    df['cluster'] = cluster_labels
    print(df.groupby('cluster')['Age'].mean())
    sns.boxplot(x='cluster', y="Age", hue="Gender", palette=["c", "m"], data=df)
    """
        boxplotì€ ì¤‘ì•™ê°’, í‘œì¤€ í¸ì°¨ ë“±, ë¶„í¬ì˜ ê°„ëµí•œ íŠ¹ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
        ê° ì¹´í…Œê³ ë¦¬ ê°’ì— ë”°ë¥¸ ë¶„í¬ì˜ ì‹¤ì œ ë°ì´í„°ì™€ í˜•ìƒì„ ë³´ê³  ì‹¶ë‹¤ë©´ violinplot, stripplot, swarmplot ë“±ìœ¼ë¡œ ì‹œê°í™” í•´ë´…ë‹ˆë‹¤.
        violinplotì€ ì„¸ë¡œ ë°©í–¥ìœ¼ë¡œ ì»¤ë„ ë°€ë„ íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë ¤ì¤ë‹ˆë‹¤. ì–‘ìª½ì´ ì™¼ìª½, ì˜¤ë¥¸ìª½ ëŒ€ì¹­ì´ ë˜ë„ë¡ í•˜ì—¬ ë°”ì´ì˜¬ë¦°ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤.
        violinplot: http://seaborn.pydata.org/generated/seaborn.violinplot.html
        swarmplotì€ stripplotê³¼ ìœ ì‚¬í•˜ë©° ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì ì´ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì˜†ìœ¼ë¡œ ì´ë™í•´ì„œ ê·¸ë ¤ì¤ë‹ˆë‹¤.
        swarmplot: http://seaborn.pydata.org/generated/seaborn.swarmplot.html
    """

    df['cluster'] = cluster_labels
    print(df.tail())

    # ê° ê·¸ë£¹ì˜ íŠ¹ì„±ì„ í™•ì¸í•˜ê¸°
    print(df.groupby('cluster')['Age'].mean())

    fig, axes = plt.subplots(2, 2, sharey=False, tight_layout=True, figsize=(15, 6), num='mobile game')
    # violinplot
    sns.violinplot(x='cluster', y='Annual Income (k$)', data=df, inner=None, ax=axes[0, 0])
    sns.swarmplot(x='cluster', y="Annual Income (k$)", data=df, ax=axes[0, 0], color='white', edgecolor='gray')
    sns.boxplot(x='cluster', y="Annual Income (k$)", data=df, ax=axes[0, 1])

    sns.violinplot(x='cluster', y='Spending Score (1-100)', data=df, inner=None, ax=axes[1, 0])
    sns.swarmplot(x='cluster', y="Spending Score (1-100)", data=df, ax=axes[1, 0], color='white', edgecolor='gray')

    sns.violinplot(x='cluster', y='Age', data=df, inner=None, ax=axes[1, 1])
    sns.swarmplot(x='cluster', y="Age", data=df, ax=axes[1, 1], color='white', edgecolor='gray')
    plt.show()

    # 3ê°œì˜ ì‹œê°í™”ë¥¼ í•œ í™”ë©´ì— ë°°ì¹˜í•©ë‹ˆë‹¤.
    figure, ((ax1, ax2, ax3)) = plt.subplots(nrows=1, ncols=3)

    # ì‹œê°í™”ì˜ ì‚¬ì´ì¦ˆë¥¼ ì„¤ì •í•´ì¤ë‹ˆë‹¤.
    figure.set_size_inches(20, 6)
    # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ swarmplotì„ ì‹œê°í™”í•´ë´…ë‹ˆë‹¤.
    ax1 = sns.violinplot(x="cluster", y='Annual Income (k$)', data=df, inner=None, ax=ax1)
    ax1 = sns.swarmplot(x="cluster", y='Annual Income (k$)', data=df,
                        color="white", edgecolor="gray", ax=ax1)

    ax2 = sns.violinplot(x="cluster", y='Spending Score (1-100)', data=df, inner=None, ax=ax2)
    ax2 = sns.swarmplot(x="cluster", y='Spending Score (1-100)', data=df,
                        color="white", edgecolor="gray", ax=ax2)

    ax3 = sns.violinplot(x="cluster", y='Age', data=df, inner=None, ax=ax3)
    ax3 = sns.swarmplot(x="cluster", y='Age', data=df,
                        color="white", edgecolor="gray", ax=ax3, hue="Gender")
    plt.show()
    """
       
        ì¶”ê°€ ë¶„ì„ì„ í•´ë³¸ë‹¤ë©´
        "Gender" ë³€ìˆ˜ í™œìš©
        K-meansëŠ” ê¸°ë³¸ì ìœ¼ë¡œ numerical variableì„ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì•¼í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        Gender ë³€ìˆ˜ë¥¼ one-hot-encodingí•˜ì—¬ ìˆ«ìë¡œ ë°”ê¿”ì¤€ ë’¤ ë³€ìˆ˜ë¡œ ì¶”ê°€í•˜ì—¬ í™œìš©í•´ë´…ë‹ˆë‹¤.
        
        ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ê°€ ëŒ€ë¶€ë¶„ì¸ ê²½ìš°ì˜ êµ°ì§‘í™”
        k-modes ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        https://pypi.org/project/kmodes/
        
    """


# marketing_03()


def marketing_04():
    """
        subject
            Machine_Running
        topic
            ex4. marketing_data
        content
            04. Revenue
        Describe
            ê³ ê° í•´ì§€ìœ¨ì„ ë‚®ì¶”ê³  CLVë¥¼ ë†’ì—¬ë³´ì
        sub Contents
            01. ë¶„ì„í•  ë°ì´í„° íŒŒì•… ( í†µì‹ ì‚¬ ê³ ê° ë°ì´í„° EDA )

            í•´ì§€ ì—¬ë¶€
                Churn - ê³ ê°ì´ ì§€ë‚œ 1ê°œì›” ë™ì•ˆ í•´ì§€í–ˆëŠ”ì§€ ì—¬ë¶€ (Yes or No)

            Demographic ì •ë³´
                customerID - ê³ ê°ë“¤ì—ê²Œ ë°°ì •ëœ ìœ ë‹ˆí¬í•œ ê³ ê° ë²ˆí˜¸ ì…ë‹ˆë‹¤.
                gender - ê³ ê°ì˜ ì„±ë³„ ì…ë‹ˆë‹¤(male or a female).
                Age - ê³ ê°ì˜ ë‚˜ì´ ì…ë‹ˆë‹¤.
                SeniorCitizen - ê³ ê°ì´ senior ì‹œë¯¼ì¸ì§€ ì—¬ë¶€(1, 0).
                Partner - ê³ ê°ì´ íŒŒíŠ¸ë„ˆê°€ ìˆëŠ”ì§€ ì—¬ë¶€(Yes, No).
                Dependents - ê³ ê°ì´ dependentsê°€ ìˆëŠ”ì§€ ì—¬ë¶€(Yes, No).

            ê³ ê°ì˜ ê³„ì • ì •ë³´
                tenure - ê³ ê°ì´ ìì‚¬ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•œ ê°œì›” ìˆ˜.
                Contract - ê³ ê°ì˜ ê³„ì•½ ê¸°ê°„ (Month-to-month, One year, Two year)
                PaperlessBilling - ê³ ê°ì´ paperless billingë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì—¬ë¶€ (Yes, No)
                PaymentMethod - ê³ ê°ì˜ ì§€ë¶ˆ ë°©ë²• (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))
                MonthlyCharges - ê³ ê°ì—ê²Œ ë§¤ì›” ì²­êµ¬ë˜ëŠ” ê¸ˆì•¡
                TotalCharges - ê³ ê°ì—ê²Œ ì´ ì²­êµ¬ëœ ê¸ˆì•¡

            ê³ ê°ì´ ê°€ì…í•œ ì„œë¹„ìŠ¤
                PhoneService - ê³ ê°ì´ ì „í™” ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì—¬ë¶€(Yes, No).
                MultipleLines - ê³ ê°ì´ multiple lineì„ ì‚¬ìš©í•˜ëŠ”ì§€ ì—¬ë¶€(Yes, No, No phone service).
                InternetService - ê³ ê°ì˜ ì¸í„°ë„· ì„œë¹„ìŠ¤ ì‚¬ì—…ì (DSL, Fiber optic, No).
                OnlineSecurity - ê³ ê°ì´ online security ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì—¬ë¶€ (Yes, No, No internet service)
                OnlineBackup - ê³ ê°ì´ online backupì„ ì‚¬ìš©í•˜ëŠ”ì§€ ì—¬ë¶€ (Yes, No, No internet service)
                DeviceProtection - ê³ ê°ì´ device protectionì— ê°€ì…í–ˆëŠ”ì§€ ì—¬ë¶€ (Yes, No, No internet service)
                TechSupport ê³ ê°ì´ tech supportë¥¼ ë°›ê³ ìˆëŠ”ì§€ ì—¬ë¶€ (Yes, No, No internet service)
                StreamingTV - ê³ ê°ì´ streaming TV ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì—¬ë¶€ (Yes, No, No internet service)
                StreamingMovies - ê³ ê°ì´ streaming movies ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì—¬ë¶€ (Yes, No, No internet service)

            ë¬¸ì œ ì •ì˜
                ë¶„ì„ì˜ ëª©ì 
                í†µì‹ ì‚¬ì˜ ê³ ê° ë°ì´í„°ì—ì„œ CLVë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
                í†µì‹ ì‚¬ ê³ ê°ì˜ churn í•´ì§€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """

    print("\n", "=" * 5, "04", "=" * 5)
    df = pd.read_csv("./data_file/WA_Fn-UseC_-Telco-Customer-Churn.csv")


    # ë°ì´í„° í™•ì¸
    print(df.shape)
    print(df.tail())

    # ê²°ì¸¡ê°’ ì¸¡ì •
    print(df.info())
    # object - Yes Or No ( Category Data )
    # TotalCharges      7043 non-null   object
    print(df.isnull().sum())

    # df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])
    print(df[df['TotalCharges'] == ' '])
    df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)
    print(df[df['tenure'] == 0])
    df = df[df['TotalCharges'].notnull()]

    # floatìœ¼ë¡œ ë³€í™˜
    df['TotalCharges'] = df['TotalCharges'].astype(float)
    print(df.info(0))

    # ê¸°ìˆ í†µê³„ í™•ì¸ : df.describe()
    print(df.describe())


    # ë³€ìˆ˜ê°„ì˜ correlation í™•ì¸ df.corr() ì‹œê°í™”
    fig, axes = plt.subplots(2, 4, sharey=False, tight_layout=True, figsize=(15, 6), num='mobile game')

    corr = df.corr()
    print(corr)
    # annot=True ìˆ«ì í‘œì‹œí•´ì¤Œ

    ax_temp = axes[0, 0]
    ax_temp.set_title('HeatMap')
    sns.heatmap(corr, annot=True, ax=ax_temp)

    # í•´ì§€í•œ ê³ ê°ìˆ˜ ( Count Flot )
    sns.countplot(y='Churn', data=df, ax=axes[0, 1])

    # ë³€ìˆ˜ê°„ì˜ pairplot
    """
        Pairplotìœ¼ë¡œ ëˆˆìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ” ê´€ê³„ëŠ” ì´ ì •ë„ ì…ë‹ˆë‹¤.
        tenureê°€ ë‚®ì€ ê²½ìš° churnì´ ë§ìŠµë‹ˆë‹¤. ì¦‰, ìµœê·¼ ê³ ê°ë“¤ì´ ë” ë§ì´ í•´ì§€í•©ë‹ˆë‹¤.
        ì–´ëŠì •ë„ ì´ìƒì˜ tenureì´ ë˜ë©´ ì¶©ì„±ê³ ê°ì´ ë˜ì–´ churní•˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.
        MonthlyChargesê°€ ë†’ì€ ê²½ìš°ì˜ churnì´ ë§ìŠµë‹ˆë‹¤.
        tenureê³¼ MonthlyChargesê°€ ì•„ë§ˆë„ ì£¼ìš”í•œ ë³€ìˆ˜ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
        scatter plotì„ ë´ë„ ì–´ëŠ ì •ë„ ê²½ê³„ì„ ì´ ë³´ì…ë‹ˆë‹¤.
        pairplotìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ” ê´€ê³„ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.
        numeric variableì´ ë§ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        categorical variableì„ ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤.
    """
    sns.pairplot(data=df, hue='Churn', markers='+', palette='husl')

    # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ í•©ë‹ˆë‹¤.
    print(df['gender'].value_counts())
    print("=================================")
    print(df['Partner'].value_counts())
    print("=================================")
    print(df['Dependents'].value_counts())
    print("=================================")
    print(df['PhoneService'].value_counts())
    print("=================================")
    print(df['MultipleLines'].value_counts())
    print("=================================")
    print(df['InternetService'].value_counts())
    print("=================================")
    print(df['OnlineSecurity'].value_counts())
    print("=================================")
    print(df['OnlineBackup'].value_counts())
    print("=================================")
    print(df['DeviceProtection'].value_counts())
    print("=================================")
    print(df['TechSupport'].value_counts())
    print("=================================")
    print(df['StreamingTV'].value_counts())
    print("=================================")
    print(df['StreamingMovies'].value_counts())
    print("=================================")
    print(df['Contract'].value_counts())
    print("=================================")
    print(df['PaperlessBilling'].value_counts())
    print("=================================")
    print(df['PaymentMethod'].value_counts())
    print("=================================")
    print(df['Churn'].value_counts())

    # ë‹¤ìŒ ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ 'No internet service'ë¥¼ 'No'ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.
    replace_cols = ['MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
                    'StreamingMovies']
    for i in replace_cols:
        df[i] = df[i].replace({'No internet service': 'No'})



    def barplot_percentages(feature, axes, orient='v', axis_name="percentage of customers"):
        ratios = pd.DataFrame()
        g = df.groupby(feature)["Churn"].value_counts().to_frame()
        g = g.rename({"Churn": axis_name}, axis=1).reset_index()
        g[axis_name] = g[axis_name] / len(df)
        if orient == 'v':
            sns.barplot(x=feature, y=axis_name, hue='Churn', data=g, orient=orient, ax=axes)
            axes.set_yticklabels(['{:,.0%}'.format(y) for y in axes.get_yticks()])
        else:
            sns.barplot(x=axis_name, y=feature, hue='Churn', data=g, orient=orient , ax=axes)
            axes.set_xticklabels(['{:,.0%}'.format(x) for x in axes.get_xticks()])
        axes.plot()

    # https://www.kaggle.com/jsaguiar/exploratory-analysis-with-seaborn

    # "SeniorCitizen"
    # SeniotCitizenì€ ì „ì²´ ê³ ê°ì˜ 16% ì •ë„ì— ë¶ˆê³¼í•˜ì§€ë§Œ churn ë¹„ìœ¨ì€ í›¨ì”¬ ë†’ìŠµë‹ˆë‹¤. (42% vs 23%)
    fig, axes = plt.subplots(2, 4, sharey=False, tight_layout=True, figsize=(15, 6), num='data bar plot')
    barplot_percentages('SeniorCitizen', axes[0, 0])

    # 'Dependents'
    # Dependentê°€ ì—†ëŠ” ê²½ìš° churnì„ ë” ë§ì´ í•©ë‹ˆë‹¤.
    barplot_percentages('Dependents', axes[0, 1])

    # 'Partner'
    # Partnerê°€ ì—†ëŠ” ê²½ìš° churnì„ ë” ë§ì´ í•©ë‹ˆë‹¤
    barplot_percentages('Partner', axes[0, 2])

    # "MultipleLines"
    # phone serviceë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê³ ê°ì˜ ë¹„ìœ¨ì€ ì ìŠµë‹ˆë‹¤.
    # MultipleLinesë¥¼ ì‚¬ìš©ì¤‘ì¸ ê³ ê°ì˜ churn ë¹„ìœ¨ì´ ì•„ì£¼ ì•½ê°„ ë†’ìŠµë‹ˆë‹¤.
    barplot_percentages('MultipleLines', axes[0, 3])

    # "InternetService"
    # ì¸í„°ë„·ì„œë¹„ìŠ¤ê°€ ì—†ëŠ” ê²½ìš°ì˜ churn ë¹„ìœ¨ì€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.
    # Fiber oppticì„ ì‚¬ìš©ì¤‘ì¸ ê³ ê°ì´ DSL ì‚¬ìš©ì¤‘ì¸ ê³ ê°ë“¤ë³´ë‹¤ churn ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤.
    barplot_percentages('InternetService', axes[1, 0])

    # 6ê°œì˜ ë¶€ê°€ ì„œë¹„ìŠ¤ê´€ë ¨ ì‹œê°í™” í•´ë´…ë‹ˆë‹¤.
    # "OnlineSecurity", "OnlineBackup", "DeviceProtection", "TechSupport" ë¶€ê°€ì„œë¹„ìŠ¤ ì‚¬ìš©ìëŠ” churn í•˜ëŠ” ê²½ìš°ê°€ ì ìŠµë‹ˆë‹¤.
    # ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ ì´ìš© ê³ ê° ì¤‘ churnì´ ë§ì€ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ("StreamingTV", "StreamingMovies")
    cols = ["OnlineSecurity", "OnlineBackup", "DeviceProtection"]
    df1 = pd.melt(df[df["InternetService"] != "No"][cols]).rename({'value': 'Has service'}, axis=1)
    sns.countplot(data=df1, x='variable', hue='Has service', ax=axes[1, 1])
    axes[1, 1].set(xlabel='Additional service', ylabel='Num of customers')

    cols = ["TechSupport", "StreamingTV", "StreamingMovies"]
    df1 = pd.melt(df[df["InternetService"] != "No"][cols]).rename({'value': 'Has service'}, axis=1)
    sns.countplot(data=df1, x='variable', hue='Has service', ax=axes[1, 2])
    axes[1, 1].set(xlabel='Additional service', ylabel='Num of customers')
    plt.show()

    # Contract ìœ í˜•ì— ë”°ë¥¸ ì›”ì²­êµ¬ìš”ê¸ˆê³¼ í•´ì§€ì—¬ë¶€ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤.
    # ì¥ê¸°ê³„ì•½ì´ê³  ì›”ì²­êµ¬ìš”ê¸ˆì´ ë†’ì„ìˆ˜ë¡ í•´ì§€ìœ¨ì´ ë†’ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
    # ì „ë°˜ì ìœ¼ë¡œ ì›”ì²­êµ¬ìš”ê¸ˆì´ ë†’ì„ë•Œ í•´ì§€ê°€ëŠ¥ì„±ì´ ë†’ì•„ë³´ì…ë‹ˆë‹¤.
    fig, axes = plt.subplots(2, 2, sharey=False, tight_layout=True, figsize=(15, 6), num='data bar plot')

    sns.boxplot(data=df, x='Contract', y='MonthlyCharges', hue='Churn', ax=axes[0, 0])
    axes[0, 0].legend(loc='center left', bbox_to_anchor=(1, 0.5))

    # PaymentMethod ìœ í˜•ì— ë”°ë¥¸ ì›”ì²­êµ¬ìš”ê¸ˆê³¼ í•´ì§€ì—¬ë¶€ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤.
    # Mailed checkëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì›”ì²­êµ¬ìš”ê¸ˆì´ ë‚®ìŠµë‹ˆë‹¤.
    # Mailed checkì—ì„œ í•´ì§€ê³ ê°ê³¼ ë¹„í•´ì§€ ê³ ê°ì˜ ì°¨ì´ê°€ í½ë‹ˆë‹¤.

    sns.boxplot(data=df, x='PaymentMethod', y='MonthlyCharges', hue='Churn', ax=axes[0, 1])
    axes[0, 1].legend(loc='center left', bbox_to_anchor=(1, 0.5))

    # tenureì— ë”°ë¥¸ ê³ ê°ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    print(df['tenure'].value_counts().sort_index())
    a = df['tenure'].value_counts().sort_index()
    print(a.shape)

    # tenureì— ë”°ë¥¸ ê³ ê°ìˆ˜ë¥¼ ì‹œê°í™”
    # 6ê°œì›” ì´í›„ retentionì´ ìƒë‹¹íˆ ë‚®ì•„ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ë°˜ë©´, ì¥ê¸° ì¶©ì„±ê³ ê°ë“¤ì€ 70ê°œì›” ì´ìƒ ìœ ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì†Œì¤‘í•œ ê³ ê°ë“¤ì…ë‹ˆë‹¤.
    ax_temp = axes[1, 0]
    ax_temp.plot(np.arange(1, 73), a, 'o')
    ax_temp.plot(np.arange(1, 73), a, '-', alpha=0.8)
    ax_temp.set_xlabel('tenure')
    ax_temp.set_ylabel('Number of customer')

    plt.show()

    print("\n", "=" * 3, "01.", "=" * 3)

    # CLV ê³„ì‚° ë° í™œìš©ë°©ì•ˆ
    # CACì™€ í•¨ê»˜ ë´ì•¼í•˜ëŠ” LTV

    """
        CLV(Customer Lifetime Value; LTV)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
            CLVëŠ” ê³ ê°ìƒì•  ê°€ì¹˜ë¥¼ ì´ì•¼ê¸° í•©ë‹ˆë‹¤.
            ê³ ê°ì´ í™•ë³´ëœ ì´í›„ ìœ ì§€ë˜ëŠ” ê¸°ê°„ë™ì•ˆì˜ ê°€ì¹˜ì…ë‹ˆë‹¤.
            CACì™€ LTVëŠ” ë°˜ë“œì‹œ íŠ¸ë˜í‚¹í•´ì•¼í•  ì£¼ìš” ì§€í‘œë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            CACë³´ë‹¤ LTVê°€ ìµœì†Œ 3ë°° ì´ìƒ ë†’ì€ ê²ƒì´ ì´ìƒì ì…ë‹ˆë‹¤.
            LTV (Lifetime value)
            
        LTV (Lifetime value)    
            ê³ ê°ë‹¹ ì›” í‰ê·  ì´ìµ(Avg monthly revenue per customer) x í‰ê·  ê³ ê° ìœ ì§€ê°œì›” ìˆ˜(# months customer lifetime)
            ê³ ê°ë‹¹ ì›” í‰ê·  ì´ìµ(Avg monthly revenue per customer) / ì›” í‰ê·  í•´ì§€ìœ¨(Monthly churn)
            (Average Value of a Sale) x (Number of Repeat Transactions) x (Average Retention Time in Months or Years for a Typical Customer)
            PLC(ì œí’ˆìˆ˜ëª…ì£¼ê¸°; Product Life Cycle) x ARPU(ê³ ê°í‰ê· ë§¤ì¶œ; Average Revenue Per User)
            ê³ ê°ë‹¹ ì›” í‰ê·  ì´ìµ(Avg Monthly Revenue per Customer x ê³ ê°ë‹¹ ë§¤ì¶œ ì´ ì´ìµ (Gross Margin per Customer) / ì›”í‰ê·  í•´ì§€ìœ¨ (Monthly Churn Rate)
            
        CAC (Customer Acquisition Cost)
            ì „ì²´ ì„¸ì¼ì¦ˆ ë§ˆì¼€íŒ… ë¹„ìš© (Total sales and marketing exppense) / # ì‹ ê·œí™•ë³´ ê³ ê° ìˆ˜ (# New customers acquired)
            LTC:CAC Ratio
    
        LTV/CAC
            1:1 ë” ë§ì´ íŒ”ìˆ˜ë¡ ë” ë§ì´ ìƒê²Œ ë©ë‹ˆë‹¤.
            3:1 ì´ìƒì ì¸ ë¹„ìœ¨ì…ë‹ˆë‹¤. (ë„ë©”ì¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
            4:1 ì¢‹ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì…ë‹ˆë‹¤.
            5:1 ë§ˆì¼€íŒ…ì— íˆ¬ìë¥¼ ëœ í•˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
    """

    # * LTV (Lifetime value)
    #  - ê³ ê°ë‹¹ ì›” í‰ê·  ì´ìµ(Avg monthly revenue per customer) x í‰ê·  ê³ ê° ìœ ì§€ê°œì›” ìˆ˜(# months customer lifetime)
    # LTVëŠ” 2100ë‹¬ëŸ¬ì…ë‹ˆë‹¤.
    # CACëŠ” 700ë‹¬ëŸ¬ ì •ë„ì¸ ê²ƒì´ ì´ìƒì ì…ë‹ˆë‹¤.
    # í†µì‹ ì‚¬ì˜ CACëŠ” ê¸°ê¸° ë³´ì¡°ê¸ˆ, ë©¤ë²„ì‹­ í˜œíƒ ë“±ì´ ìˆìŠµë‹ˆë‹¤.

    ltv = df['MonthlyCharges'].mean() * df['tenure'].mean()
    print('LTV : ', ltv)
    print("\n", "=" * 3, "02.", "=" * 3)

    # Churn í•´ì§€í•  ê³ ê°ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    df2 = df.iloc[:, 1:]
    print(df2.tail())

    # target Value Churn Yes / No -> 1 / 0
    # binary í˜•íƒœì˜ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ë¥¼ numeric variableë¡œ ë³€ê²½í•´ì¤ë‹ˆë‹¤.
    df2['Churn'].replace(to_replace='Yes', value=1, inplace=True)
    df2['Churn'].replace(to_replace='No', value=0, inplace=True)

    # ëª¨ë“  categorical ë³€ìˆ˜ë¥¼ ë”ë¯¸ ë³€ìˆ˜í™” ì‹œí‚µë‹ˆë‹¤.
    df_dummies = pd.get_dummies(df2)
    print(df_dummies.shape, df_dummies.tail())

    # dummy ë³€ìˆ˜í™”í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    y = df_dummies['Churn'].values
    X = df_dummies.drop(columns='Churn')

    # ë³€ìˆ˜ ê°’ì„ 0ê³¼ 1ì‚¬ì´ ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ í•´ì¤ë‹ˆë‹¤.
    from sklearn.preprocessing import MinMaxScaler
    features = X.columns.values
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaler.fit(X)
    X = pd.DataFrame(scaler.transform(X))
    X.columns = features
    print(X.shape)
    print(X.tail())

    # Create Train & Test Data
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

    # logistic regression
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(max_iter=1000)
    result = model.fit(X_train, y_train)

    from sklearn import metrics
    prediction_test = model.predict(X_test)
    # Print the prediction accuracy
    print(metrics.accuracy_score(y_test, prediction_test))

    print("\n", "=" * 3, "03.", "=" * 3)

    # ëª¨ë“  ë³€ìˆ˜ì˜ weights ê°’ì„ ê°€ì ¸ì™€ì„œ ì‹œê°í™” í•©ë‹ˆë‹¤.
    weights = pd.Series(model.coef_[0], index=X.columns.values)
    plt.rcParams['figure.figsize'] = (20, 4)
    weights.sort_values(ascending=False).plot(kind='bar')

    """
        ê²°ê³¼ í•´ì„ ë° ì ìš© ë°©ì•ˆ
            ë°ì´í„° íƒìƒ‰ê³¼ì •ì—ì„œ ì£¼ìš”í•œ ë³€ìˆ˜ì¼ ê²ƒìœ¼ë¡œ ë³´ì˜€ë˜ ë³€ìˆ˜ë“¤ì˜ weightê°€ ì‹¤ì œë¡œ ë†’ìŠµë‹ˆë‹¤.
            trnureê°€ ê¸¸ìˆ˜ë¡ ì¶©ì„±ê³ ê°ì˜ churnì€ ë‚®ì•„ì§‘ë‹ˆë‹¤. tenureê°€ ì•„ì£¼ ê¸´ ìœ ì €ë“¤ì˜ churnì´ ë‚®ì€ ê²ƒì´ ì´ë ‡ê²Œ ë‚˜íƒ€ë‚œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. (TotalChargesëŠ” ë°˜ëŒ€ë¡œ ê°™ì€ì´ì¹˜)
            ì¸í„°ë„· ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì€ ê³ ê°ì˜ churnì„ ì¤„ì…ë‹ˆë‹¤.
            Fiber optic ì¸í„°ë„· ì„œë¹„ìŠ¤ ì‚¬ìš©ê³¼ ì›”ë‹¨ìœ„ ê³„ì•½, Electronic Checkë¥¼ ì‚¬ìš©í•˜ëŠ” ê³ ê°ì¼ìˆ˜ë¡ churnì´ ë†’ì•„ì§‘ë‹ˆë‹¤.
    """
    # RandomForest
    from sklearn.ensemble import RandomForestClassifier
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
    model_rf = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1,
                                      random_state=50, max_features="auto",
                                      max_leaf_nodes=30)
    model_rf.fit(X_train, y_train)

    # Make predictions
    prediction_test = model_rf.predict(X_test)
    print(metrics.accuracy_score(y_test, prediction_test))

    importances = model_rf.feature_importances_
    weights = pd.Series(importances, index=X.columns.values)
    plt.rcParams["figure.figsize"] = (14, 4)
    weights.sort_values(ascending=False).plot(kind='bar')

    # random forest ì•Œê³ ë¦¬ì¦˜ì—ì„œ monthly contract, tenure and total chargesê°€ churnì„ ì˜ˆì¸¡í•˜ëŠ” ê°€ì¥ ì£¼ìš”í•œ ë³€ìˆ˜ì…ë‹ˆë‹¤.
    # logistic regressionì˜ ê²°ê³¼ì™€ EDA ê²°ê³¼ì™€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤.

    """
    ì ìš© ë°©ì•ˆ
        ì¤‘ìš”ë„ê°€ ë†’ì€ ë³€ìˆ˜ë¥¼ í™œìš©í•œ ë§ˆì¼€íŒ… ì „ëµì„ ìˆ˜ë¦½í•´ë´…ë‹ˆë‹¤.
        ê³„ì•½ ì¡°ê±´ì„ ë³€ê²½í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 2ë…„ ì¥ê¸°ê³„ì•½ì„ ìµœëŒ€í•œ ìœ ë„í•´ë´…ë‹ˆë‹¤.
        í° ë³´ì¡°ê¸ˆì„ ë§ì´ ì§€ê¸‰í•´ì„œ CACê°€ ë†’ì•„ì§€ë”ë¼ë„ ì¥ê¸°ì ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ LTVë¥¼ ë†’ì¸ë‹¤ë©´ í†µì‹ ì‚¬ì—ê²Œ ë” ìœ ë¦¬í•©ë‹ˆë‹¤.
        Fiber opptic ì„ ì‚¬ìš©í• ìˆ˜ë¡ í•´ì§€í™•ë¥ ì´ ë†’ì•„ì§€ëŠ”ë°, ê·¸ ì´ìœ ë¥¼ ì°¾ì•„ë´…ë‹ˆë‹¤. ì¸í„°ë„· í†µì‹  í†µí•©ìš”ê¸ˆì œ ë“±ì˜ ì˜í–¥ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        etc
        ë§¤ë‹¬ ê³ ê°ë³„ churnì„ ì˜ˆì¸¡í•˜ì—¬ churní•  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë˜ëŠ” ê³ ê°ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ ì„ í–‰ì  ì¡°ì¹˜ë¥¼ ì·¨í•©ë‹ˆë‹¤.
        ì˜ˆ: ìƒˆ ê¸°ê¸°ë¡œ êµì²´í•´ì£¼ê³  ë³´ì¡°ê¸ˆì„ ì§€ê¸‰í•œ ë’¤ 2ë…„ ê³„ì•½í•˜ëŠ” ìª½ìœ¼ë¡œ ìœ ë„í•˜ëŠ” ë§ˆì¼€íŒ… ì „í™”ë¥¼ ëŒë ¤ë´…ë‹ˆë‹¤.
    """

    from sklearn.svm import SVC
    model.svm = SVC(kernel='linear')
    model.svm.fit(X_train, y_train)
    preds = model.svm.predict(X_test)
    metrics.accuracy_score(y_test, preds)

    # Create the Confusion matrix
    from sklearn.metrics import classification_report, confusion_matrix
    print(confusion_matrix(y_test, preds))

    # ADA Boost (AdaBoost Algorithm)
    from sklearn.ensemble import AdaBoostClassifier
    model = AdaBoostClassifier()
    # n_estimators = 50 (default value)
    # base_estimator = DecisionTreeClassifier (default value)
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    metrics.accuracy_score(y_test, preds)

    # XG Boost
    from xgboost import XGBClassifier
    model = XGBClassifier()
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    metrics.accuracy_score(y_test, preds)


# marketing_04()


def marketing_05():
    """
        subject
            Machine_Running
        topic
            ex4. marketing_data
        content
            05. Referral
        Describe
            ë¦¬ë·° ë¶„ì„ì„ í†µí•˜ ì†Œë¹„ì ì¡°ì‚¬
        sub Contents
            01. ë¶„ì„í•  ë°ì´í„° íŒŒì•…, ê²½ìŸì‚¬ ê³ ê° ë¦¬ë·°

            STEP1. í˜•íƒœì†Œ ë¶„ì„
            STEP2. ë¶ˆìš©ì–´ ì²˜ë¦¬

        í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì„ ìœ„í•œ ì „ì²˜ë¦¬
            KoNLPë¥¼ ì´ìš©í•œ í˜•íƒœì†Œ ë¶„ì„
            KoNLPyê°€ ì œê³µí•˜ëŠ” í˜•íƒœì†Œë¶„ì„ê¸° ì¤‘ í•˜ë‚˜ì¸ Kkmaë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            ìì„¸í•œ ë‚´ìš©ì€ http://konlpy.org/ko/v0.4.3/morph/ ì°¸ì¡°
        í˜•íƒœì†Œ ë¶„ì„ê¸°
            í•œë‚˜ëˆ” http://semanticweb.kaist.ac.kr/hannanum/index.html
            íŠ¸ìœ„í„° https://github.com/twitter/twitter-korean-text
            ê¼¬ê¼¬ë§ˆ http://kkma.snu.ac.kr/documents/
    """

    print("\n", "=" * 5, "05", "=" * 5)
    from konlpy.tag import Hannanum
    from konlpy.tag import Twitter
    from konlpy.tag import Kkma
    hannanum = Hannanum()
    twitter = Twitter()
    kkma = Kkma()

    """
        ê¼¬ê¼¬ë§ˆ í˜•íƒœì†Œ ë¶„ì„ê¸°
            ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³  í’ˆì‚¬ë¥¼ íƒœê¹…í•©ë‹ˆë‹¤
            í’ˆì‚¬íƒœê·¸ëŠ” ì¼ë°˜ëª…ì‚¬(NNG), ê³ ìœ ëª…ì‚¬(NNP), ë™ì‚¬(VV), í˜•ìš©ì‚¬(VA) ë“±ì´ ìˆìŠµë‹ˆë‹¤
            http://kkma.snu.ac.kr/documents/index.jsp?doc=postag í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸
    """
    print(kkma.sentences(u'ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ê°€ ë°© ì•ˆì— ìˆëŠ” ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤.'))
    print(kkma.pos(u'ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ê°€ ë°© ì•ˆì— ìˆëŠ” ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤.'))
    print(hannanum.pos(u'ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ê°€ ë°© ì•ˆì— ìˆëŠ” ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤.'))
    print(twitter.pos('ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤. ì•„ë²„ì§€ê°€ ë°© ì•ˆì— ìˆëŠ” ê°€ë°©ì— ë“¤ì–´ê°€ì…¨ë‹¤.'))

    print("\n", "=" * 3, "01.", "=" * 3)
    """
        í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë¶„ì„ ë° ì‹œê°í™”
            ì„¼íŠ¸ë£¸ ë°ì´í„°ë¥¼ ë¨¼ì € ë¶„ì„í•´ë´…ë‹ˆë‹¤.
    """

    line_list = []
    f = open("data_file/centrum_review.txt", encoding="utf-8")
    for line in f:
        line = kkma.nouns(line)
        line_list.append(line)
    f.close()
    print("- ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ :", len(line_list), "ë¬¸ì¥")

    word_frequency = {}
    noun_list = []
    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    stop_list = ["ë°°ì†¡", "ë§Œì¡±", 'êµ¬ë§¤', 'ê°ì‚¬']
    line_number = 0
    for line in line_list[:]:
        line_number += 1
        print(str(line_number) + "/" + str(len(line_list)), end="\r")
        noun = []
        for word in line:
            if word.split("/")[0] not in stop_list and len(word.split("/")[0]) > 1:
                noun.append(word.split("/")[0])
                if word not in word_frequency.keys():
                    word_frequency[word] = 1
                else:
                    word_frequency[word] += 1
        noun_list.extend(noun)

    # ë‹¨ì–´ë³„ ì¶œí˜„ë¹ˆë„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    word_count = []
    for n, freq in word_frequency.items():
        word_count.append([n, freq])
    word_count.sort(key=lambda elem: elem[1], reverse=True)
    for n, freq in word_count[:10]:
        print(n + "\t" + str(freq))
    # ì¶”ì¶œí•œ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ í™œìš©í•´ ëª…ì‚¬ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    noun_doc = ' '.join(noun_list)
    noun_doc = noun_doc.strip()


    """
        ì„œì²´ ë‹¤ìš´ë¡œë“œ
            ì‹œê°í™”ì—ì„œ ì„œì²´ ë³€ê²½ë§Œìœ¼ë¡œë„ ì™„ì„±ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            ë‹¤ìŒì˜ ë§í¬ì—ì„œ ë‚˜ëˆ”ìŠ¤í€˜ì–´ ì„œì²´ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì£¼ì„¸ìš”.
            ì°¸ê³ : https://hangeul.naver.com/2017/nanum
    """
    # ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
    # pip install wordcloud

    from wordcloud import WordCloud, ImageColorGenerator
    import matplotlib.pyplot as plt

    # í°íŠ¸ì—…ë¡œë“œ

    # ì›Œë“œí´ë¼ìš°ë“œ íŒŒë¼ë¯¸í„° ì„¤ì •
    font_path = "font/NanumSquareB.otf"  # í°íŠ¸
    background_color = "white"  # ë°°ê²½ìƒ‰
    margin = 3  # ëª¨ì„œë¦¬ ì—¬ë°± ë„“ì´
    min_font_size = 7  # ìµœì†Œ ê¸€ì í¬ê¸°
    max_font_size = 150  # ìµœëŒ€ ê¸€ì í¬ê¸°
    width = 500  # ì´ë¯¸ì§€ ê°€ë¡œ í¬ê¸°
    height = 500  # ì´ë¯¸ì§€ ì„¸ë¡œ í¬ê¸°
    wc = WordCloud(font_path=font_path, background_color=background_color, margin=margin, \
                   min_font_size=min_font_size, max_font_size=max_font_size, width=width, height=height)
    wc.generate(noun_doc)

    plt.figure(figsize=(10, 10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()

    print("\n", "=" * 3, "02.", "=" * 3)
    """
        í…ìŠ¤íŠ¸ì—ì„œ í† í”½/ì£¼ì œ ì°¾ê¸°
        LDA í† í”½ ëª¨ë¸ë§
    """

    # pip install gensim
    # pip install corpora
    # pip install wheel
    import gensim
    from gensim import corpora
    import logging
    logging.basicConfig(level=logging.DEBUG)
    topic = 5
    keyword = 10
    texts = []
    resultList = []
    stop_list = ["ë°°ì†¡", "ë§Œì¡±", "ì¹´í˜", "ì¹´í˜ê·œì •", "í™•ì¸", "ì£¼ìˆ˜", "ì„¼íŠ¸"]
    for line in line_list:
        words = line
        if words != [""]:
            tokens = [word for word in words if (len(word.split("/")[0]) > 1 and word.split("/")[0] not in stop_list)]
            texts.append(tokens)
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]

    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=topic, id2word=dictionary, passes=10)
    for num in range(topic):
        resultList.append(ldamodel.show_topic(num, keyword))
    print("\n", "=" * 3, "03.", "=" * 3)

    print(resultList)
    # unsupervide running ì´ë¼ì„œ ë¶„ì„í• ë•Œë§ˆë‹¤ ê²°ê³¼ê°€ ì¢€ ë‹¬ë¼ì§.
    # gephi ì‹œê°í™”ë„ í•´ë³´ë©´ ì¢‹ìŒ..


marketing_05()


def marketing_temp():
    """
        subject
            Machine_Running
        topic
            ex4. marketing_data
        content
            temp
        Describe

        sub Contents
            01.
    """

    print("\n", "=" * 5, "temp", "=" * 5)

    print("\n", "=" * 3, "01.", "=" * 3)
    print("\n", "=" * 3, "02.", "=" * 3)
    print("\n", "=" * 3, "03.", "=" * 3)

# marketing_temp()

